"""
FaceNetäººè„¸è¯†åˆ«è®­ç»ƒè„šæœ¬
ä½¿ç”¨facenet-pytorchæå–äººè„¸ç‰¹å¾,è®­ç»ƒSVMåˆ†ç±»å™¨
"""
import cv2  # å…ˆå¯¼å…¥cv2é¿å…DLLé—®é¢˜
import numpy as np
import pickle
import sys
from pathlib import Path

# æ·»åŠ backendç›®å½•åˆ°è·¯å¾„
backend_dir = Path(__file__).parent.parent.parent
sys.path.insert(0, str(backend_dir))

from sklearn.svm import SVC
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import torch
from facenet_pytorch import InceptionResnetV1
import mediapipe as mp
from config import config
from train.common import YOLOFaceDetector, load_dataset_by_class
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class FaceNetTrainer:
    """FaceNetäººè„¸è¯†åˆ«è®­ç»ƒå™¨"""
    
    def __init__(
        self,
        dataset_dir: str = "dataset",
        yolo_model_path: str = None,
        device: str = None
    ):
        """
        åˆå§‹åŒ–è®­ç»ƒå™¨
        
        Args:
            dataset_dir: æ•°æ®é›†ç›®å½•
            yolo_model_path: YOLOæ¨¡å‹è·¯å¾„
            device: è®¾å¤‡
        """
        self.dataset_dir = Path(dataset_dir)
        
        # è®¾ç½®è®¾å¤‡
        if device is None:
            self.device = config.get_device()
        else:
            self.device = torch.device(device)
        
        logger.info(f"ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        # åˆå§‹åŒ–YOLOæ£€æµ‹å™¨
        if yolo_model_path is None:
            yolo_model_path = str(config.YOLO_MODEL)
        
        self.detector = YOLOFaceDetector(
            model_path=yolo_model_path,
            confidence_threshold=config.YOLO_CONFIDENCE_THRESHOLD
        )
        
        # åˆå§‹åŒ–MediaPipeäººè„¸å…³é”®ç‚¹æ£€æµ‹(ç”¨äºå¯¹é½)
        logger.info("åŠ è½½MediaPipeäººè„¸å¯¹é½æ¨¡å‹...")
        self.mp_face_mesh = mp.solutions.face_mesh.FaceMesh(
            static_image_mode=True,
            max_num_faces=1,
            refine_landmarks=True,
            min_detection_confidence=0.5
        )
        logger.info("âœ“ MediaPipeæ¨¡å‹å·²åŠ è½½")
        
        # åˆå§‹åŒ–FaceNetæ¨¡å‹(ä½¿ç”¨é¢„è®­ç»ƒæƒé‡)
        logger.info("åŠ è½½FaceNetæ¨¡å‹...")
        self.facenet = InceptionResnetV1(pretrained='vggface2').eval().to(self.device)
        logger.info("âœ“ FaceNetæ¨¡å‹å·²åŠ è½½")
        
        self.X = []  # ç‰¹å¾å‘é‡
        self.y = []  # æ ‡ç­¾
        self.label_encoder = LabelEncoder()
        self.svm_model = None
    
    def align_face(self, face_image: np.ndarray) -> np.ndarray:
        """
        ä½¿ç”¨MediaPipeè¿›è¡Œäººè„¸å¯¹é½
        
        Args:
            face_image: BGRæ ¼å¼äººè„¸å›¾åƒ
        
        Returns:
            å¯¹é½åçš„äººè„¸å›¾åƒ
        """
        try:
            # è½¬RGB
            face_rgb = cv2.cvtColor(face_image, cv2.COLOR_BGR2RGB)
            
            # æ£€æµ‹å…³é”®ç‚¹
            results = self.mp_face_mesh.process(face_rgb)
            
            if not results.multi_face_landmarks:
                return face_image  # æœªæ£€æµ‹åˆ°å…³é”®ç‚¹,è¿”å›åŸå›¾
            
            # è·å–å…³é”®ç‚¹
            landmarks = results.multi_face_landmarks[0]
            h, w = face_image.shape[:2]
            
            # æå–çœ¼ç›å…³é”®ç‚¹(ç”¨äºå¯¹é½)
            # å·¦çœ¼: 33, å³çœ¼: 263
            left_eye = landmarks.landmark[33]
            right_eye = landmarks.landmark[263]
            
            # è®¡ç®—çœ¼ç›ä¸­å¿ƒç‚¹
            left_eye_pt = np.array([left_eye.x * w, left_eye.y * h])
            right_eye_pt = np.array([right_eye.x * w, right_eye.y * h])
            
            # è®¡ç®—æ—‹è½¬è§’åº¦
            dY = right_eye_pt[1] - left_eye_pt[1]
            dX = right_eye_pt[0] - left_eye_pt[0]
            angle = np.degrees(np.arctan2(dY, dX))
            
            # è®¡ç®—çœ¼ç›ä¸­å¿ƒ
            eyes_center = ((left_eye_pt[0] + right_eye_pt[0]) / 2,
                          (left_eye_pt[1] + right_eye_pt[1]) / 2)
            
            # è·å–æ—‹è½¬çŸ©é˜µ
            M = cv2.getRotationMatrix2D(eyes_center, angle, 1.0)
            
            # æ—‹è½¬å›¾åƒ
            aligned = cv2.warpAffine(face_image, M, (w, h),
                                    flags=cv2.INTER_CUBIC,
                                    borderMode=cv2.BORDER_REPLICATE)
            
            return aligned
            
        except Exception as e:
            logger.debug(f"äººè„¸å¯¹é½å¤±è´¥: {e}, ä½¿ç”¨åŸå›¾")
            return face_image
    
    def extract_face_embedding(self, face_image: np.ndarray, use_alignment: bool = True) -> np.ndarray:
        """
        æå–äººè„¸åµŒå…¥å‘é‡
        
        Args:
            face_image: BGRæ ¼å¼äººè„¸å›¾åƒ
            use_alignment: æ˜¯å¦ä½¿ç”¨MediaPipeå¯¹é½
        
        Returns:
            512ç»´åµŒå…¥å‘é‡
        """
        # 1. äººè„¸å¯¹é½
        if use_alignment:
            face_image = self.align_face(face_image)
        
        # 2. BGR to RGB
        face_rgb = cv2.cvtColor(face_image, cv2.COLOR_BGR2RGB)
        
        # 3. è°ƒæ•´å¤§å°åˆ°160x160
        face_resized = cv2.resize(face_rgb, config.FACE_SIZE)
        
        # 4. è½¬æ¢ä¸ºtensorå¹¶å½’ä¸€åŒ–
        face_tensor = torch.from_numpy(face_resized).float()
        face_tensor = face_tensor.permute(2, 0, 1)  # HWC -> CHW
        face_tensor = (face_tensor - 127.5) / 128.0  # å½’ä¸€åŒ–åˆ°[-1, 1]
        face_tensor = face_tensor.unsqueeze(0).to(self.device)  # æ·»åŠ batchç»´åº¦
        
        # 5. æå–ç‰¹å¾
        with torch.no_grad():
            embedding = self.facenet(face_tensor)
        
        # 6. è½¬æ¢ä¸ºnumpy
        embedding = embedding.cpu().numpy().flatten()
        
        return embedding
    
    def augment_face(self, face_image: np.ndarray) -> list:
        """
        æ•°æ®å¢å¼º:ç”Ÿæˆè½»å¾®å˜æ¢çš„äººè„¸å›¾åƒ
        
        Args:
            face_image: åŸå§‹äººè„¸å›¾åƒ
        
        Returns:
            å¢å¼ºåçš„äººè„¸å›¾åƒåˆ—è¡¨
        """
        augmented = [face_image]  # åŒ…å«åŸå›¾
        
        h, w = face_image.shape[:2]
        
        # è½»å¾®æ—‹è½¬ (-5, +5åº¦)
        for angle in [-5, 5]:
            M = cv2.getRotationMatrix2D((w/2, h/2), angle, 1.0)
            rotated = cv2.warpAffine(face_image, M, (w, h))
            augmented.append(rotated)
        
        # äº®åº¦è°ƒæ•´
        hsv = cv2.cvtColor(face_image, cv2.COLOR_BGR2HSV).astype(np.float32)
        hsv[:, :, 2] = hsv[:, :, 2] * 1.1  # å¢äº®10%
        hsv[:, :, 2] = np.clip(hsv[:, :, 2], 0, 255)
        brightened = cv2.cvtColor(hsv.astype(np.uint8), cv2.COLOR_HSV2BGR)
        augmented.append(brightened)
        
        hsv[:, :, 2] = hsv[:, :, 2] * 0.9  # é™æš—10%
        darkened = cv2.cvtColor(hsv.astype(np.uint8), cv2.COLOR_HSV2BGR)
        augmented.append(darkened)
        
        return augmented
    
    def load_and_process_dataset(self):
        """åŠ è½½å¹¶å¤„ç†æ•°æ®é›†"""
        logger.info("=" * 60)
        logger.info("åŠ è½½æ•°æ®é›†...")
        logger.info("=" * 60)
        
        if not self.dataset_dir.exists():
            logger.error(f"æ•°æ®é›†ç›®å½•ä¸å­˜åœ¨: {self.dataset_dir}")
            return False
        
        # è·å–æ‰€æœ‰ç”¨æˆ·æ–‡ä»¶å¤¹
        user_dirs = [d for d in self.dataset_dir.iterdir() if d.is_dir()]
        
        if len(user_dirs) == 0:
            logger.error("æœªæ‰¾åˆ°ç”¨æˆ·æ•°æ®")
            return False
        
        logger.info(f"æ‰¾åˆ° {len(user_dirs)} ä¸ªç”¨æˆ·")
        
        # å¤„ç†æ¯ä¸ªç”¨æˆ·
        for user_dir in user_dirs:
            user_folder_name = user_dir.name
            
            # ğŸ”§ å…³é”®ä¿®æ”¹ï¼šå°è¯•å°†æ–‡ä»¶å¤¹åè½¬æ¢ä¸ºæ•°å­—ID
            # å¦‚æœæ–‡ä»¶å¤¹åæ˜¯æ•°å­—ï¼Œä½¿ç”¨æ•°å­—IDï¼ˆè½¬ä¸ºå­—ç¬¦ä¸²ï¼‰
            # å¦‚æœä¸æ˜¯æ•°å­—ï¼Œä½¿ç”¨æ–‡ä»¶å¤¹åï¼ˆå­—ç¬¦ä¸²ï¼‰
            try:
                user_id = int(user_folder_name)
                user_label = str(user_id)  # ç»Ÿä¸€è½¬ä¸ºå­—ç¬¦ä¸²ç±»å‹
                logger.info(f"\nå¤„ç†ç”¨æˆ·: {user_folder_name} -> ID: {user_label} (æ•°å­—ID)")
            except ValueError:
                user_label = user_folder_name
                logger.warning(f"\nå¤„ç†ç”¨æˆ·: {user_folder_name} (å­—ç¬¦ä¸²ç”¨æˆ·å - ä¸æ¨è)")
                logger.warning(f"  âš ï¸  å»ºè®®ä½¿ç”¨æ•°å­—ä½œä¸ºæ–‡ä»¶å¤¹åï¼Œä¾‹å¦‚: 1, 2, 3...")
            
            # ç›´æ¥åŠ è½½è¯¥ç”¨æˆ·ç›®å½•ä¸‹çš„æ‰€æœ‰å›¾åƒ
            from train.common.data_utils import load_face_images
            images = load_face_images(user_dir)
            
            if len(images) == 0:
                logger.warning(f"  âš  ç”¨æˆ· {user_label} æ²¡æœ‰å›¾åƒ,è·³è¿‡")
                continue
            
            # æå–æ¯å¼ å›¾åƒçš„åµŒå…¥å‘é‡(å¸¦æ•°æ®å¢å¼º)
            embeddings = []
            for i, img in enumerate(images):
                try:
                    # æ£€æµ‹äººè„¸
                    face = self.detector.detect_single_face(img)
                    
                    if face is None:
                        logger.warning(f"  âš  å›¾åƒ {i+1} æœªæ£€æµ‹åˆ°äººè„¸,è·³è¿‡")
                        continue
                    
                    # æ•°æ®å¢å¼º(æ¯å¼ åŸå›¾ç”Ÿæˆå¤šä¸ªå˜ä½“)
                    augmented_faces = self.augment_face(face)
                    
                    # å¯¹æ¯ä¸ªå¢å¼ºæ ·æœ¬æå–ç‰¹å¾(ä½¿ç”¨MTCNNå¯¹é½)
                    for aug_face in augmented_faces:
                        embedding = self.extract_face_embedding(aug_face, use_alignment=True)
                        embeddings.append(embedding)
                    
                except Exception as e:
                    logger.warning(f"  âš  å¤„ç†å›¾åƒ {i+1} å¤±è´¥: {e}")
            
            if len(embeddings) > 0:
                self.X.extend(embeddings)
                # ğŸ”§ ä½¿ç”¨ç»Ÿä¸€çš„å­—ç¬¦ä¸²ç±»å‹label
                self.y.extend([user_label] * len(embeddings))
                logger.info(f"  âœ“ æˆåŠŸå¤„ç† {len(embeddings)}/{len(images)} å¼ å›¾åƒ (Label: '{user_label}')")
            else:
                logger.warning(f"  âš  ç”¨æˆ· {user_label} æ²¡æœ‰æœ‰æ•ˆå›¾åƒ")
        
        # è½¬æ¢ä¸ºnumpyæ•°ç»„
        self.X = np.array(self.X)
        # ğŸ”§ ç¡®ä¿labelsæ˜¯objectç±»å‹ï¼ˆå­—ç¬¦ä¸²ï¼‰
        self.y = np.array(self.y, dtype=object)
        
        logger.info("\n" + "=" * 60)
        logger.info(f"æ•°æ®é›†åŠ è½½å®Œæˆ:")
        logger.info(f"  - æ€»æ ·æœ¬æ•°: {len(self.X)}")
        logger.info(f"  - ç”¨æˆ·æ•°: {len(set(self.y))}")
        logger.info(f"  - ç”¨æˆ·IDåˆ—è¡¨: {np.unique(self.y)}")
        logger.info(f"  - Labelsç±»å‹: {self.y.dtype}")
        logger.info(f"  - ç‰¹å¾ç»´åº¦: {self.X.shape[1]}")
        logger.info("=" * 60)
        
        return len(self.X) > 0
    
    def train_svm(self, test_size: float = 0.2):
        """è®­ç»ƒSVMåˆ†ç±»å™¨"""
        logger.info("\nå¼€å§‹è®­ç»ƒSVMåˆ†ç±»å™¨...")
        
        # ç¼–ç æ ‡ç­¾
        y_encoded = self.label_encoder.fit_transform(self.y)
        
        # åˆ†å‰²æ•°æ®é›†
        X_train, X_test, y_train, y_test = train_test_split(
            self.X,
            y_encoded,
            test_size=test_size,
            random_state=42,
            stratify=y_encoded
        )
        
        logger.info(f"è®­ç»ƒé›†å¤§å°: {len(X_train)}")
        logger.info(f"æµ‹è¯•é›†å¤§å°: {len(X_test)}")
        
        # è®­ç»ƒSVM
        logger.info("\nè®­ç»ƒä¸­...")
        self.svm_model = SVC(kernel='linear', probability=True, random_state=42)
        self.svm_model.fit(X_train, y_train)
        
        # è¯„ä¼°
        y_pred = self.svm_model.predict(X_test)
        accuracy = accuracy_score(y_test, y_pred)
        
        logger.info("\n" + "=" * 60)
        logger.info("è®­ç»ƒå®Œæˆ!")
        logger.info(f"æµ‹è¯•é›†å‡†ç¡®ç‡: {accuracy * 100:.2f}%")
        logger.info("=" * 60)
        
        # è¯¦ç»†æŠ¥å‘Š
        logger.info("\nåˆ†ç±»æŠ¥å‘Š:")
        logger.info("\n" + classification_report(
            y_test,
            y_pred,
            target_names=self.label_encoder.classes_
        ))
        
        # æ··æ·†çŸ©é˜µ
        logger.info("æ··æ·†çŸ©é˜µ:")
        cm = confusion_matrix(y_test, y_pred)
        logger.info(f"\n{cm}")
        
        return accuracy
    
    def save_model(self):
        """ä¿å­˜æ¨¡å‹å’ŒåµŒå…¥æ•°æ®"""
        logger.info("\nä¿å­˜æ¨¡å‹...")
        
        # ä¿å­˜åµŒå…¥å‘é‡å’Œæ ‡ç­¾
        embeddings_path = config.FACENET_EMBEDDINGS
        # ğŸ”§ ä½¿ç”¨allow_pickle=Trueä»¥æ”¯æŒobjectç±»å‹çš„labels
        np.savez_compressed(
            embeddings_path,
            embeddings=self.X,
            labels=self.y
        )
        logger.info(f"âœ“ åµŒå…¥æ•°æ®å·²ä¿å­˜: {embeddings_path}")
        logger.info(f"  - Labelsç±»å‹: {self.y.dtype}")
        logger.info(f"  - ç”¨æˆ·IDåˆ—è¡¨: {np.unique(self.y)}")
        
        # ğŸ”§ å…³é”®ä¿®æ”¹ï¼šç›´æ¥ä¿å­˜SVMæ¨¡å‹å¯¹è±¡ï¼Œä¸ä½¿ç”¨å­—å…¸
        # è¿™æ ·ä¸ç³»ç»Ÿæ³¨å†Œä¿å­˜çš„æ ¼å¼ä¸€è‡´
        svm_path = config.FACENET_SVM
        
        with open(svm_path, 'wb') as f:
            pickle.dump(self.svm_model, f)
        
        logger.info(f"âœ“ SVMæ¨¡å‹å·²ä¿å­˜: {svm_path}")
        logger.info(f"  - æ¨¡å‹ç±»å‹: {type(self.svm_model).__name__}")
        logger.info(f"  - ç±»åˆ«æ•°: {len(self.svm_model.classes_)}")
        logger.info("\næ‰€æœ‰æ¨¡å‹æ–‡ä»¶å·²ä¿å­˜!")


def main():
    """ä¸»è®­ç»ƒæµç¨‹"""
    logger.info("=" * 60)
    logger.info("FaceNet äººè„¸è¯†åˆ«è®­ç»ƒ")
    logger.info("=" * 60)
    
    # åˆå§‹åŒ–è®­ç»ƒå™¨
    trainer = FaceNetTrainer(dataset_dir="dataset")
    
    # åŠ è½½æ•°æ®é›†
    if not trainer.load_and_process_dataset():
        logger.error("æ•°æ®é›†åŠ è½½å¤±è´¥")
        return
    
    # è®­ç»ƒSVM
    accuracy = trainer.train_svm(test_size=0.2)
    
    # ä¿å­˜æ¨¡å‹
    trainer.save_model()
    
    logger.info("\n" + "=" * 60)
    logger.info("è®­ç»ƒæµç¨‹å®Œæˆ!")
    logger.info(f"æœ€ç»ˆå‡†ç¡®ç‡: {accuracy * 100:.2f}%")
    logger.info("=" * 60)


if __name__ == '__main__':
    main()
